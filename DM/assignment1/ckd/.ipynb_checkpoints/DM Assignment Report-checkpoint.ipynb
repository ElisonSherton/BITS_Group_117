{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Introduction\n",
    "\n",
    "Build a binary classification model to predict if a person has `chronic kidney disease(ckd)` or he does not have that disease.\n",
    "\n",
    "The dataset has\n",
    "\n",
    "- 400 total datapoints\n",
    "- 11 numerical features\n",
    "- 13 categorical/nominal features\n",
    "- 1 Target variable represented by the feature name `class`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thought Process\n",
    "\n",
    "We have a dataset of 400 datapoints and we need to build a binary classification model for a target called `class`. Here are the steps we intend to perform to tackle the problem at hand.\n",
    "\n",
    "- Understand the underlying distribution of different features and get a descriptive summary of the dataset\n",
    "- Perform cleanup of data\n",
    "- Engineer some features which can give good performance in classification\n",
    "- Build several ML Models with different estimators\n",
    "- Evaluate the model holistically\n",
    "- Explain future scope and what could be improved upon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing/ Descriptive Summarization\n",
    "\n",
    "Here are a few things which we observed in the dataset\n",
    "\n",
    "- There are several missing values in the dataset for both categorical and numerical variables.\n",
    "- Features are not normally distributed and there are several potential outliers in the dataset.\n",
    "- Numerical features have skewed distributions which are multimodal in nature.\n",
    "- Categorical features have class imbalances in them\n",
    "\n",
    "How did we handle them?\n",
    "\n",
    "- Impute numerical columns using knn imputation\n",
    "- Impute categorical columns using `most_frequent` observation. \n",
    "\n",
    "***Why imputation? Why not drop the features/rows altogether?***\n",
    "\n",
    "The dataset at hand has only 400 rows, dropping it any further would mean much less data than what we already have.\n",
    "\n",
    "Also, we don't have too many features to worry about curse of dimensionality, so we need not drop the columns as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "**Blood Pressure / Age**\n",
    "\n",
    "We can construct a feature for blood pressure per unit age of the person. This will be an informative metric as far as the health of the individual is concerned i.e. to understand whether based on age the BP is high or low.\n",
    "\n",
    "**hemoglobin / rbcc**\n",
    "\n",
    "The proportion of hemoglobin to that of red blood cell counts is one more key information that can tell about the resistance power of an individual. Hemoglobin with iron must be high and also red blood cells count must also be high. This ratio will tell us the relative proportion of each of them in an individual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building\n",
    "\n",
    "There is a plethora of models to choose from for the usecase at hand i.e. `binary classification` problem. However, we will restrict ourselves to the models which we have studied in the lectures. For being ambitious, we will also fit a model which is not covered in lecture but a natural extension of one of the models taught in the lectures.\n",
    "\n",
    "1. Logistic Regression - Generally, this is the first choice of modelling any problem whichi nvolves classification. We can get a good baseline using this method. It is a high bias, low variance method.\n",
    "\n",
    "2. Decision Tree - This models the problem as a set of if-else rules which partitions the space orthogonally into parallelopipeds in the n-feature hyperspace. These are easy to build and have high variance.\n",
    "\n",
    "3. Random Forests - A natural extension of decision trees which employ the power of ensembling. They build a lot of weak classifiers and combine them to make a strong classification model.\n",
    "\n",
    "These three models are available in `sklearn.linear_model`, `sklearn.tree` and `sklearn.ensemble` packages respectively which we use for building three varieties of models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "Once we have our model, we need a way to tell how good it's performance is. This is what we refer to as model evaluation and there are some very robust metrics which we can use to assess our model.\n",
    "\n",
    "1. Accuracy - Fraction of correct predictions overall.\n",
    "$$Accuracy = \\frac{TP + TN}{TP + FP + TN + FN}$$\n",
    "\n",
    "2. Recall - How many examples of the positive class was the model able to identify correctly\n",
    "$$Recall = \\frac{TP }{TP + FN}$$\n",
    "\n",
    "3. Precision - Out of all predicted positives, in how many of them was the model actually correct?\n",
    "$$Precision = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "4. F1-Score- To strike the goldilocks' balance between precision and recall.\n",
    "$$F_1Score = \\frac{2 \\times Precision \\times Recall}{Precision + Recall}$$\n",
    "\n",
    "When the classes in target are balanced, accuracy is as good a metric as any but as in our example where we have imbalanced datasets we are interested more in being always correct when identifying `CKD` as opposed to misidentifying it i.e. if we are able to identify all `CKD` patients to be actually `CKD` patients, that is more important than identifying some `non-CKD` person to have `CKD` because in latter, we can confirm and let them go away later on. However the risk is high when a `CKD` person is identified to be `non-CKD`. \n",
    "\n",
    "This means we need to find out precision and recall and strike a balance between them (Well, recall should be more but having good precision is also important).\n",
    "\n",
    "With our models, we observed the following performance on train validation and test sets respectively\n",
    "\n",
    "*Accuracy Chart*\n",
    "\n",
    "||Train|Valid|Test|\n",
    "|--|--|--|--|\n",
    "|Logistic Reg|100|100|96.67%|\n",
    "|Decision Tree|100|100|96.67%|\n",
    "|Random Forest|100|100|96.67%|\n",
    "\n",
    "*F1 Chart*\n",
    "\n",
    "||Train|Valid|Test|\n",
    "|--|--|--|--|\n",
    "|Logistic Reg|100|100|96.47%|\n",
    "|Decision Tree|100|100|96.41%|\n",
    "|Random Forest|100|100|96.41%|\n",
    "\n",
    "This is an example of baseline being pretty strong to break so to speak. But I will go with either Decision Tree or Random Forest over logistic Regression still. Why? Because if we look at the confusion matrix of test set for these two, we get a 100% recall whereas that isn't true for Logistic Regression.\n",
    "\n",
    "I really care a lot about identifying all the `CKD` patients with a higher success ratio as opposed to `non-CKD` ones. That's the logic behind this decision. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Scope/ Refinement opportunities\n",
    "\n",
    "1. The dataset is an imbalanced dataset. We can introduce synthetic oversampling techniques such as SMOTE / Undersampling/ Oversampling to balance the dataset and then create a model.\n",
    "\n",
    "2. The number of datapoints is pretty low and amongst them, we can see that the number of missing values is high. We have used knn imputation currently, but we could also use mean/median/other imputation techniques and compare their performance.\n",
    "\n",
    "3. Use of neural networks and deep learning (Multi-Layer Perceptron) models for the analysis of this problem.\n",
    "\n",
    "4. Domain understanding for even better feature engineering to milk the predictive performance of the model even further."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
